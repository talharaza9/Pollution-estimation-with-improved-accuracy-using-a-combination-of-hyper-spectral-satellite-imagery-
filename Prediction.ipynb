{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "798f4b23-07f5-44b7-bfae-fa0fceb822b0",
   "metadata": {},
   "source": [
    "# Pollution estimation with improved accuracy using a combination of hyper-spectral satellite imagery data and maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaa0fd4-3217-45af-9192-a6ff035a2d3b",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ce2142-055f-46bc-8e83-42ea0641af49",
   "metadata": {},
   "source": [
    "#### Using satellite imagery along with the map data from the open street map to find the regression estimate of pollution (Nitrogen Dioxide) in the region for specific time instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2b9d7a-6bd8-4b96-8864-2aa270d26f97",
   "metadata": {},
   "source": [
    "### Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad084d54-955e-4e7a-a98e-4ff3bfc0573d",
   "metadata": {},
   "source": [
    "This hackathon Is very challenging and interesting at the same time because we not only have to make it computationally efficient but also preserve features importance. Considering so many features for regression model, it was difficult task to develop a model that deals so huge sets of features. We divided our project into two parts. First part was data processing and compression, and second part was modelling. Data were extracted from NetCDF format using Panoply. Extracted data consisted of data for whole world. We had to extract information from that, so we made a function which captures the nearest coordinates (to coordinates given by user) and return features vector of that particular location and then converting them to data frame and then finally to array which can be used by model.\n",
    " Our approach was to break model into three parts or take three sets of factors into account. Each model takes a definite set of features. These models are then merge together to get a hybrid neural network architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c314d6-000b-4c31-abf3-5470af74f1da",
   "metadata": {},
   "source": [
    "![Image](hack.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "b4b25a49-6c2d-4ce2-a70c-6876c2adeb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model 2 feature extraction\n",
    "import numpy as np\n",
    "import geopy.distance\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import smopy\n",
    "import cv2\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input,concatenate\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import keras,os\n",
    "from keras.models import Sequential\n",
    "#from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d5cc6f-ea11-4224-8f09-22035d05e182",
   "metadata": {},
   "source": [
    "## Important functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252f36fa-fa3e-4844-9095-30c8bb1bd086",
   "metadata": {},
   "source": [
    "#### NetCDF data contains data from whole world. coordinates of a location is accessed by spanline and groundpixel. so this function will return spanline and ground pixel of netcdf data which can be further used to get feautes of particular location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "ba1b2163-0f26-4e97-b784-a42c3c60ac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span_grid(latitude,longitude):\n",
    "    distance_between={}\n",
    "    a=latitude\n",
    "    b=longitude\n",
    "    for i in coord_local.columns:\n",
    "        for j in coord_local.index:\n",
    "            coords_1=(a,b)\n",
    "            try:\n",
    "                coords_2=(float(coord_local[i][j].split(',')[0]),float(coord_local[i][j].split(',')[1]))\n",
    "                dist_calc=geopy.distance.distance(coords_1, coords_2).km\n",
    "                #print(dist_calc)\n",
    "                distance_between[float(dist_calc)]=(i,j)\n",
    "            except AttributeError:\n",
    "                continue\n",
    "    \n",
    "    grid,span=distance_between[min(distance_between.keys())]\n",
    "    return grid,span"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca610a41-8d87-478c-886c-70bd98384c4b",
   "metadata": {},
   "source": [
    "#### Compressing coordinates data for london and nearby. Since having coordinates for each location of world is not feasable and make data size huge, so this function will narrow down coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "b70e71a3-fef1-4231-9c91-c09b561ead71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract(coordinates):\n",
    "    latitude_1=50.0\n",
    "    latitude_2=52\n",
    "    longitude_1=-1\n",
    "    longitude_2=2\n",
    "    lat=float(coordinates.split(',')[0])\n",
    "    lon=float(coordinates.split(',')[1])\n",
    "    if lat > latitude_1 and lat < latitude_2 and lon > longitude_1 and lon < longitude_2:\n",
    "        return str(lat)+','+str(lon)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e128ff-e3d2-4f61-a582-3ca088f36980",
   "metadata": {},
   "source": [
    "#### Getting features of satellite data through this function which automatically takes coordinates using get_span_grid fuunction to get spanline and ground pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e4c375-a540-4e3a-b0c1-a627d5db96f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyper_Spectral_features(lat,lon):\n",
    "    ground_pixel,span_line=get_span_grid(lat,lon)\n",
    "    feature1=air_mass_factor_troposphere.iloc[span_line][ground_pixel]\n",
    "    feature2=cloud_radiance_fraction_nitrogendioxide_window.iloc[span_line][ground_pixel]\n",
    "    feature3=nitrogendioxide_tropospheric_column.iloc[span_line][ground_pixel]\n",
    "    feature4=water_liquid_slant_column_density.iloc[span_line][ground_pixel]\n",
    "    hyper_spectral_features=[feature1,feature2,feature3,feature4]\n",
    "    return hyper_spectral_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75463f9-0b3b-4c81-b1ec-fb9efefe5f33",
   "metadata": {},
   "source": [
    "#### Converting input time to suitable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "adcff831-fd7c-4a69-8f26-083b923c60f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(date_string):\n",
    "    convert_date=datetime.strptime(date_string, '%B %d %Y %H:%M%p')\n",
    "    convert_date=convert_date.strftime('%d/%m/%Y %H:%M:%S')\n",
    "    convert_date=datetime.strptime(convert_date, '%d/%m/%Y %H:%M:%S')\n",
    "    return convert_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ce048-5a15-4eeb-9fd8-ad83a1971f94",
   "metadata": {},
   "source": [
    "#### Helper function to remove bad data which cannot be converted to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "608a1536-ebee-4443-a1f9-9f3a4075e17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bad_data(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "        \n",
    "    except ValueError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04945f9b-6a4d-4091-a847-04912ba152c4",
   "metadata": {},
   "source": [
    "#### To get nearest coordinates for time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "80255c49-d6dc-4db8-b9f8-c2185a25709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coord_pointer(lat,lon):\n",
    "    coord_1=(lat,lon)\n",
    "    array_coord = os.listdir('data1/timeseries')\n",
    "    dist=10000\n",
    "    for i in array_coord:\n",
    "        coord_2=(float(i.split('_')[0]),i.split('_')[1].split('.csv')[0])\n",
    "        distance=geopy.distance.distance(coord_1, coord_2).km\n",
    "        if distance <dist:\n",
    "            dist=distance\n",
    "            coord_pointer=i\n",
    "            \n",
    "    return coord_pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "a5eb194d-2f27-4766-8956-819e73ccb924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get distance also\n",
    "def get_coord_pointer_dist(lat,lon):\n",
    "    coord_1=(lat,lon)\n",
    "    array_coord = os.listdir('data1/timeseries')\n",
    "    dist=10000\n",
    "    for i in array_coord:\n",
    "        coord_2=(float(i.split('_')[0]),i.split('_')[1].split('.csv')[0])\n",
    "        distance=geopy.distance.distance(coord_1, coord_2).km\n",
    "        if distance <dist:\n",
    "            dist=distance\n",
    "            coord_pointer=i\n",
    "            \n",
    "    return coord_pointer,dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f57255-3745-42b9-9cd4-3f1b7ca8b8d8",
   "metadata": {},
   "source": [
    "## Extracting and compressing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3490358f-0177-4739-9110-b56b003af20e",
   "metadata": {},
   "source": [
    "##### Note: Do not run ths because this was raw data and then later it compressed. So if you want to explore compressed data skip this section. you can also look folder features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "ea8a8ba8-933b-4a16-903c-29dc59b61a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>441</th>\n",
       "      <th>442</th>\n",
       "      <th>443</th>\n",
       "      <th>444</th>\n",
       "      <th>445</th>\n",
       "      <th>446</th>\n",
       "      <th>447</th>\n",
       "      <th>448</th>\n",
       "      <th>449</th>\n",
       "      <th>450</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3730</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.996674</td>\n",
       "      <td>4.035570</td>\n",
       "      <td>4.064984</td>\n",
       "      <td>4.067941</td>\n",
       "      <td>4.033320</td>\n",
       "      <td>3.947936</td>\n",
       "      <td>3.814722</td>\n",
       "      <td>3.649008</td>\n",
       "      <td>3.642426</td>\n",
       "      <td>...</td>\n",
       "      <td>2.635669</td>\n",
       "      <td>2.613383</td>\n",
       "      <td>2.593173</td>\n",
       "      <td>2.573021</td>\n",
       "      <td>2.544679</td>\n",
       "      <td>2.507278</td>\n",
       "      <td>2.471988</td>\n",
       "      <td>2.428825</td>\n",
       "      <td>2.375102</td>\n",
       "      <td>2.327318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3731</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.014929</td>\n",
       "      <td>3.988487</td>\n",
       "      <td>3.998895</td>\n",
       "      <td>4.013307</td>\n",
       "      <td>3.969422</td>\n",
       "      <td>3.905218</td>\n",
       "      <td>3.778103</td>\n",
       "      <td>3.642178</td>\n",
       "      <td>3.592658</td>\n",
       "      <td>...</td>\n",
       "      <td>2.608702</td>\n",
       "      <td>2.600179</td>\n",
       "      <td>2.605427</td>\n",
       "      <td>2.584003</td>\n",
       "      <td>2.547207</td>\n",
       "      <td>2.507121</td>\n",
       "      <td>2.471816</td>\n",
       "      <td>2.433708</td>\n",
       "      <td>2.385399</td>\n",
       "      <td>2.337814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3732</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.931113</td>\n",
       "      <td>3.907353</td>\n",
       "      <td>3.928293</td>\n",
       "      <td>3.967877</td>\n",
       "      <td>3.971287</td>\n",
       "      <td>3.811789</td>\n",
       "      <td>3.764996</td>\n",
       "      <td>3.806537</td>\n",
       "      <td>3.805441</td>\n",
       "      <td>...</td>\n",
       "      <td>2.612480</td>\n",
       "      <td>2.616801</td>\n",
       "      <td>2.619986</td>\n",
       "      <td>2.591538</td>\n",
       "      <td>2.554092</td>\n",
       "      <td>2.504519</td>\n",
       "      <td>2.471715</td>\n",
       "      <td>2.442029</td>\n",
       "      <td>2.394749</td>\n",
       "      <td>2.357204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3733</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.926278</td>\n",
       "      <td>3.838709</td>\n",
       "      <td>3.944358</td>\n",
       "      <td>3.986444</td>\n",
       "      <td>3.971414</td>\n",
       "      <td>3.850075</td>\n",
       "      <td>3.843834</td>\n",
       "      <td>3.880197</td>\n",
       "      <td>3.857552</td>\n",
       "      <td>...</td>\n",
       "      <td>2.623158</td>\n",
       "      <td>2.625378</td>\n",
       "      <td>2.615728</td>\n",
       "      <td>2.593893</td>\n",
       "      <td>2.557548</td>\n",
       "      <td>2.511056</td>\n",
       "      <td>2.473781</td>\n",
       "      <td>2.433137</td>\n",
       "      <td>2.378527</td>\n",
       "      <td>2.351014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3734</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.902305</td>\n",
       "      <td>3.952622</td>\n",
       "      <td>3.987734</td>\n",
       "      <td>3.976619</td>\n",
       "      <td>3.948929</td>\n",
       "      <td>3.871465</td>\n",
       "      <td>3.897126</td>\n",
       "      <td>3.937358</td>\n",
       "      <td>3.951623</td>\n",
       "      <td>...</td>\n",
       "      <td>2.646542</td>\n",
       "      <td>2.635777</td>\n",
       "      <td>2.624989</td>\n",
       "      <td>2.605721</td>\n",
       "      <td>2.560582</td>\n",
       "      <td>2.510756</td>\n",
       "      <td>2.474772</td>\n",
       "      <td>2.420678</td>\n",
       "      <td>2.345738</td>\n",
       "      <td>2.306786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3735 rows Ã— 450 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       1         2         3         4         5         6         7  \\\n",
       "0    NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "1    NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2    NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "3    NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "4    NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "...   ..       ...       ...       ...       ...       ...       ...   \n",
       "3730 NaN  3.996674  4.035570  4.064984  4.067941  4.033320  3.947936   \n",
       "3731 NaN  4.014929  3.988487  3.998895  4.013307  3.969422  3.905218   \n",
       "3732 NaN  3.931113  3.907353  3.928293  3.967877  3.971287  3.811789   \n",
       "3733 NaN  3.926278  3.838709  3.944358  3.986444  3.971414  3.850075   \n",
       "3734 NaN  3.902305  3.952622  3.987734  3.976619  3.948929  3.871465   \n",
       "\n",
       "             8         9        10  ...       441       442       443  \\\n",
       "0          NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
       "1          NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
       "2          NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
       "3          NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
       "4          NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3730  3.814722  3.649008  3.642426  ...  2.635669  2.613383  2.593173   \n",
       "3731  3.778103  3.642178  3.592658  ...  2.608702  2.600179  2.605427   \n",
       "3732  3.764996  3.806537  3.805441  ...  2.612480  2.616801  2.619986   \n",
       "3733  3.843834  3.880197  3.857552  ...  2.623158  2.625378  2.615728   \n",
       "3734  3.897126  3.937358  3.951623  ...  2.646542  2.635777  2.624989   \n",
       "\n",
       "           444       445       446       447       448       449       450  \n",
       "0          NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "1          NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "2          NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "3          NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "4          NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3730  2.573021  2.544679  2.507278  2.471988  2.428825  2.375102  2.327318  \n",
       "3731  2.584003  2.547207  2.507121  2.471816  2.433708  2.385399  2.337814  \n",
       "3732  2.591538  2.554092  2.504519  2.471715  2.442029  2.394749  2.357204  \n",
       "3733  2.593893  2.557548  2.511056  2.473781  2.433137  2.378527  2.351014  \n",
       "3734  2.605721  2.560582  2.510756  2.474772  2.420678  2.345738  2.306786  \n",
       "\n",
       "[3735 rows x 450 columns]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset consits of data for all world so we need to narrow down location till U.K\n",
    "\n",
    "air_mass_factor_troposphere=pd.read_csv('data1/features/air_mass_factor_troposphere.csv')\n",
    "cloud_radiance_fraction_nitrogendioxide_window=pd.read_csv('data1/features/cloud_radiance_fraction_nitrogendioxide_window.csv')\n",
    "nitrogendioxide_tropospheric_column=pd.read_csv('data1/features/nitrogendioxide_tropospheric_column.csv')\n",
    "water_liquid_slant_column_density=pd.read_csv('data1/features/water_liquid_slant_column_density.csv')\n",
    "air_mass_factor_troposphere\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1310508-7c86-4a40-9867-ae465dc6e998",
   "metadata": {},
   "source": [
    "### Reoving coordinates of whole world except region of interest (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a8bd74-0521-4048-bf64-981307c52c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coord=pd.read_csv('data1/geographical_data/coord.csv')\n",
    "\n",
    "coord_local=coord.copy()\n",
    "for i in coord_local.columns.tolist():\n",
    "    coord_local[i]=coord_local[i].apply(lambda x: extract(x))\n",
    "    \n",
    "coord_local=coord_local.loc[~(coord_local==0).all(axis=1)]\n",
    "coord_local=coord_local.loc[~(coord_local==0).all(axis=1)]\n",
    "\n",
    "list_drop=[]\n",
    "\n",
    "for i in coord_local.columns:\n",
    "    #coord_local[i]=coord_local[i].apply(lambda x:float(x))\n",
    "    try:\n",
    "        coord_local[i]=coord_local[i].apply(lambda x:float(x))\n",
    "        \n",
    "    except ValueError:\n",
    "        continue\n",
    "    try:\n",
    "        if coord_local[i].sum()==0:\n",
    "            list_drop.append(i)\n",
    "    except TypeError:\n",
    "        continue\n",
    "coord_local=coord_local.drop(list_drop, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2328b224-8562-4f04-adb3-d804e06bdb17",
   "metadata": {},
   "source": [
    "### compressing data (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "6e9f7f75-27eb-4ed1-ad21-3a4c8d58ba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_range=list(range(75,450))\n",
    "list_final=[]\n",
    "for i in list_range:\n",
    "    list_final.append(str(i))\n",
    "    \n",
    "list_range=list(range(3110,3734))\n",
    "row_final=[]\n",
    "for i in list_range:\n",
    "    row_final.append(i)\n",
    "    \n",
    "air_mass_factor_troposphere[0:2057]=0\n",
    "air_mass_factor_troposphere[3110:]=0\n",
    "air_mass_factor_troposphere[list_final]=0\n",
    "air_mass_factor_troposphere=air_mass_factor_troposphere.drop(columns=list_final)\n",
    "air_mass_factor_troposphere=air_mass_factor_troposphere.drop(row_final)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cloud_radiance_fraction_nitrogendioxide_window[0:2057]=0\n",
    "cloud_radiance_fraction_nitrogendioxide_window[3110:]=0\n",
    "cloud_radiance_fraction_nitrogendioxide_window[list_final]=0\n",
    "cloud_radiance_fraction_nitrogendioxide_window=cloud_radiance_fraction_nitrogendioxide_window.drop(columns=list_final)\n",
    "cloud_radiance_fraction_nitrogendioxide_window=cloud_radiance_fraction_nitrogendioxide_window.drop(row_final)\n",
    "\n",
    "\n",
    "nitrogendioxide_tropospheric_column[0:2057]=0\n",
    "nitrogendioxide_tropospheric_column[3110:]=0\n",
    "nitrogendioxide_tropospheric_column[list_final]=0\n",
    "nitrogendioxide_tropospheric_column=nitrogendioxide_tropospheric_column.drop(columns=list_final)\n",
    "nitrogendioxide_tropospheric_column=nitrogendioxide_tropospheric_column.drop(row_final)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "water_liquid_slant_column_density[0:2057]=0\n",
    "water_liquid_slant_column_density[3110:]=0\n",
    "water_liquid_slant_column_density[list_final]=0\n",
    "water_liquid_slant_column_density=water_liquid_slant_column_density.drop(columns=list_final)\n",
    "water_liquid_slant_column_density=water_liquid_slant_column_density.drop(row_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4345bc82-4bb1-460a-9883-7b96fc5e9187",
   "metadata": {},
   "source": [
    "### Saving compressed data (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "ac865978-5947-4e33-b5dc-c112495203d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_mass_factor_troposphere.to_excel('air_mass_factor_troposphere.xlsx')\n",
    "cloud_radiance_fraction_nitrogendioxide_window.to_excel('cloud_radiance_fraction_nitrogendioxide_window.xlsx')\n",
    "nitrogendioxide_tropospheric_column.to_excel('nitrogendioxide_tropospheric_column.xlsx')\n",
    "water_liquid_slant_column_density.to_excel('water_liquid_slant_column_density.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ad89e-c938-4b06-879a-f5c20074c835",
   "metadata": {},
   "source": [
    "### Reading  compressed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "0e7f22b1-9162-4a1b-b1e2-5652f8d3c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading features from compressed file\n",
    "air_mass_factor_troposphere=pd.read_excel('data1/features/air_mass_factor_troposphere.xlsx')\n",
    "cloud_radiance_fraction_nitrogendioxide_window=pd.read_excel('data1/features/cloud_radiance_fraction_nitrogendioxide_window.xlsx')\n",
    "nitrogendioxide_tropospheric_column=pd.read_excel('data1/features/nitrogendioxide_tropospheric_column.xlsx')\n",
    "water_liquid_slant_column_density=pd.read_excel('data1/features/water_liquid_slant_column_density.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "efdf97e4-ede9-420e-91f2-30f717b94b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readinf coordinates(compressed) data\n",
    "coord_local.to_excel('extracted_features/coordinates.xlsx')\n",
    "c=pd.read_excel('data1/geographical_data/coordinates.xlsx')\n",
    "coord_local=c.set_index('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8ab68a-fcca-4137-b9c5-2b92931f979c",
   "metadata": {},
   "source": [
    "## size of extracted features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1459750c-baca-4d4a-bedb-4e0270010d32",
   "metadata": {},
   "source": [
    "Extracted features are stored in 'data1' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "3f5a935f-572f-4f3b-b8d0-90464b893379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4703.642kb\n"
     ]
    }
   ],
   "source": [
    "Folderpath = 'data1/features'  \n",
    " \n",
    "size = 0\n",
    "for i in os.scandir(Folderpath):\n",
    "    size+=os.path.getsize(i)\n",
    "Folderpath = 'data1/geographical_data'  \n",
    "for i in os.scandir(Folderpath):\n",
    "    size+=os.path.getsize(i)\n",
    "Folderpath = 'data1/timeseries' \n",
    "for i in os.scandir(Folderpath):\n",
    "    size+=os.path.getsize(i)\n",
    "size=size/1000\n",
    "print(str(size)+'kb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa27c8d-8e8d-49b9-9400-2aa7a149017a",
   "metadata": {},
   "source": [
    "# Training dataset\n",
    "#### Trained model already saved in my_model folder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe0bdb5-4dde-4ba0-8053-49f6bfd44df6",
   "metadata": {},
   "source": [
    "#### Processing map data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06e0af93-ea02-4f73-a2a0-802c7ad39367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowered zoom level to keep map size reasonable. (z = 11)\n"
     ]
    }
   ],
   "source": [
    "# Checking sample from map and doing analysis\n",
    "lat=51.20\n",
    "lon=-0.25\n",
    "LatMin=lat-0.1\n",
    "LatMax=lat+0.1\n",
    "LngMin=lon-0.1\n",
    "LngMax=lon+0.1\n",
    "map = smopy.Map((LatMin,LngMin, LatMax,LngMax))\n",
    "\n",
    "## Deciding na,e of photo which will be saved for further processing\n",
    "name=str(lat)+'_'+str(lon)+'.png'\n",
    "map.save_png(name)\n",
    "img = cv2.imread(name)\n",
    "\n",
    "resized_image = cv2.resize(img, (60, 60)) \n",
    "resized_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef418bf-7e89-4d7c-ad90-49cc5fa95936",
   "metadata": {},
   "source": [
    "# Model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a9f633-6681-41c1-a8f9-be553f0d3a38",
   "metadata": {},
   "source": [
    "## CNN model for map data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "937c6348-4263-49e9-bf90-53502c53cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1 using tensorflow functional API\n",
    "input1=Input(shape=(60, 60, 3)) # input shape\n",
    "y1=Conv2D(32, (3, 3), activation='relu', input_shape=(60, 60, 3))(input1)\n",
    "y2= MaxPooling2D()(y1)\n",
    "y3=Conv2D(64, (3, 3), activation='relu')(y2)\n",
    "y4= Flatten()(y3)\n",
    "output1=Dense(10, activation=\"relu\")(y4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fb35b3-c506-4d58-9172-68d9ffca3a33",
   "metadata": {},
   "source": [
    "## Model for last 7 days data and data from satellite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "e64e81a2-e428-4229-9721-c6edec1fcb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2 Last 7 days input (24x7) hourly\n",
    "input2=Input(shape=(168,))\n",
    "z1=Dense(5, activation=\"relu\")(input2)\n",
    "\n",
    "#model3 input features from satellite\n",
    "input3=Input(shape=(4,))\n",
    "x=Dense(1, activation=\"relu\")(input3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a73065-6bd1-45f3-88d2-326d69fdc961",
   "metadata": {},
   "source": [
    "## Merging all models on second last Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "a0d9a772-0a3e-42d2-bc26-1871bf893b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = concatenate([output1, z1,x])\n",
    "out=Dense(1, activation=\"relu\")(merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b869737-44cc-4d02-b69b-4503b94a6260",
   "metadata": {},
   "source": [
    "### Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "9c19abf1-66e0-40cb-9719-9951b5470aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(inputs=[input1,input2,input3],outputs=[out])\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafcccab-9206-4802-a6ef-bd629701469b",
   "metadata": {},
   "source": [
    "### Getting time series data for trainng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "3e4467c2-d87b-4616-bb83-2a07b3f10246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time series\n",
    "London_Bexley=(51.462900, 0.139400)\n",
    "df=pd.read_csv('data1/timeseries/51.462900_ 0.139400.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d89080-2098-41d1-a9b2-c653690ab8aa",
   "metadata": {},
   "source": [
    "#### Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "52054593-0ee2-4b05-ba24-81aa30656ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()\n",
    "df['date']=df['Date']+' '+df['Time']\n",
    "from datetime import datetime\n",
    "df['date'] = df['date'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "2b51cbf7-6880-4772-a3ca-75f8de83a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking function\n",
    "#given_date=%B %d %Y %H:%M%p\n",
    "given_date='March 25 2021 11:00AM'\n",
    "convert_date=datetime.strptime(given_date, '%B %d %Y %H:%M%p')\n",
    "convert_date=convert_date.strftime('%d/%m/%Y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "e2a50856-829d-408a-a9af-92aa9c5ce78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'25/03/2021 11:00:00'"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "cc5c4e07-84ac-4e31-8b5d-80512db719b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#given coordinates and date and time, extract time series and dependent variable\n",
    "import os\n",
    "arr = os.listdir('data1/timeseries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "6efb3c53-8c1d-401e-827f-f1a1ad029b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy.distance\n",
    "lat_1=51.522918\n",
    "\n",
    "lon_1=-0.130821\n",
    "coord_1=(lat_1,lon_1)\n",
    "#distgeopy.distance.distance(coords_1, coords_2).km\n",
    "\n",
    "dist=10000\n",
    "for i in arr:\n",
    "    coord_2=(float(i.split('_')[0]),i.split('_')[1].split('.csv')[0])\n",
    "    #print(coord_2)\n",
    "    distance=geopy.distance.distance(coord_1, coord_2).km\n",
    "    #print(distance)\n",
    "    if distance <dist:\n",
    "        dist=distance\n",
    "        coord_pointer=i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "2a98becd-338b-4ef5-9099-b1b6a0f5fe38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'51.522918_-0.130821.csv'"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord_pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "b4ef2b75-86c4-47b9-9813-828e4dd30823",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv('data1/timeseries/'+coord_pointer)\n",
    "df=df.dropna()\n",
    "df['date']=df['Date']+' '+df['Time']\n",
    "from datetime import datetime\n",
    "df['date'] = df['date'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "0a66802d-cfb7-49ff-8b53-4d8cbe9a6dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "given_time= 'March 26 2021 11:00AM'\n",
    "time=convert_date(given_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "25a9ba96-2878-4cc3-a1a3-c7541d4c0eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= float(df[df.date == time]['Nitrogen dioxide'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "bbe2252e-b60b-4926-9aaa-0ce7ac05ed96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 3, 19, 11, 0)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hours = 168\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "hours_added = timedelta(hours = -hours)\n",
    "time_past=time + hours_added\n",
    "#future_date_and_time = current_date_and_time + hours_added\n",
    "time_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "b02b9918-554c-4089-8871-2eac8e463fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= df[df['date'] < time]#['Nitrogen dioxide']\n",
    "x=x[x['date'] >= time_past]['Nitrogen dioxide']\n",
    "#x=x.apply(lambda y:float(y))\n",
    "x=list(x)\n",
    "X=[]\n",
    "for i in x:\n",
    "    try:\n",
    "        X.append(float(i))\n",
    "    except ValueError:\n",
    "        X.append(statistics.mean(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fa00ebc9-15c4-4071-8d9e-6404dd72b7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df =pd.read_csv('data1/timeseries/'+'51.522918_-0.130821.csv')\n",
    "df=df.dropna()\n",
    "df['date']=df['Date']+' '+df['Time']\n",
    "from datetime import datetime\n",
    "df['date'] = df['date'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "baa6465b-a129-4d33-a293-3831e3ae508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_ran=['AM','PM']\n",
    "month_ran=['March','April']\n",
    "day_ran=list(range(8,30))\n",
    "time1_ran=list(range(1,12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "f14ade0e-47e4-4dd6-a156-f362f954a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import statistics\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "given_time= random.choice(month_ran)+ ' '  + str(random.choice(day_ran))+ ' ' + '2021' + ' ' + str(random.choice(time1_ran)) +':'+'00'+random.choice(time_ran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "32ce064a-8e61-428a-bdd0-1642abe06396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowered zoom level to keep map size reasonable. (z = 12)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df =pd.read_csv('data1/timeseries/'+'51.522918_-0.130821.csv')\n",
    "df=df.dropna()\n",
    "df['date']=df['Date']+' '+df['Time']\n",
    "from datetime import datetime\n",
    "df['date'] = df['date'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S'))\n",
    "import smopy\n",
    "import cv2\n",
    "\n",
    "lat=51.522918\n",
    "lon=-0.130821\n",
    "LatMin=lat-0.1\n",
    "LatMax=lat+0.1\n",
    "LngMin=lon-0.1\n",
    "LngMax=lon+0.1\n",
    "map = smopy.Map((LatMin,LngMin, LatMax,LngMax))\n",
    "name=str(lat)+'_'+str(lon)+'.png'\n",
    "map.save_png(name)\n",
    "img = cv2.imread(name)\n",
    "resized_image = cv2.resize(img, (60, 60)) \n",
    "resized_image.shape\n",
    "spec_features=get_hyper_Spectral_features(lat,lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c6263a-f436-4d4a-aa9d-1b351ad0deba",
   "metadata": {},
   "source": [
    "### Training dataset preparation (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "6b01705e-2c89-4674-ac6f-1b721e2d9a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array=[]\n",
    "y_array=[]\n",
    "x_hyper=[]\n",
    "x_map=[]\n",
    "for i in range(15000):\n",
    "    x_map.append(resized_image)\n",
    "    x_hyper.append(spec_features)\n",
    "    temp=0\n",
    "    given_time= random.choice(month_ran)+ ' '  + str(random.choice(day_ran))+ ' ' + '2021' + ' ' + str(random.choice(time1_ran)) +':'+'00'+random.choice(time_ran)\n",
    "    time=convert_date(given_time)\n",
    "    try:\n",
    "        y= float(df[df.date == time]['Nitrogen dioxide'])\n",
    "    except ValueError:\n",
    "        y =0\n",
    "    \n",
    "    hours = 168\n",
    "    hours_added = timedelta(hours = -hours)\n",
    "    time_past=time + hours_added\n",
    "    #future_date_and_time = current_date_and_time + hours_added\n",
    "    time_past\n",
    "    x= df[df['date'] < time]#['Nitrogen dioxide']\n",
    "    x=x[x['date'] >= time_past]['Nitrogen dioxide']\n",
    "    x=list(x)\n",
    "    X=[]\n",
    "    for i in x:\n",
    "        try:\n",
    "            X.append(float(i))\n",
    "        except ValueError:\n",
    "            X.append(10) #very few cases where due to some error string is not converted to float\n",
    "            temp=X.copy()\n",
    "    y_array.append(y)\n",
    "    x_array.append(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "ea57027a-6735-4eeb-9c96-3ab5ab3ef6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8622cc67-9a59-4845-ab76-20c359eca0aa",
   "metadata": {},
   "source": [
    "# Model Training (skip as trained model available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "4fdb480a-df4b-4771-a0c3-1d73361dc826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input,concatenate\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "#from keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Model\n",
    "input1=Input(shape=(60, 60, 3)) # input layer\n",
    "y1=Conv2D(32, (3, 3), activation='relu', input_shape=(60, 60, 3))(input1)  #convolution layer\n",
    "y2= MaxPooling2D()(y1)  #maxpooling layer\n",
    "y3=Conv2D(64, (3, 3), activation='relu')(y2)\n",
    "y4= Flatten()(y3)\n",
    "output1=Dense(10, activation=\"relu\")(y4)\n",
    "\n",
    "#model2\n",
    "input2=Input(shape=(5,))\n",
    "z1=Dense(168, activation=\"relu\")(input2)\n",
    "#model3\n",
    "input3=Input(shape=(4,))\n",
    "x=Dense(1, activation=\"relu\")(input3)\n",
    "\n",
    "merge = concatenate([output1, z1,x])\n",
    "out=Dense(1, activation=\"relu\")(merge)\n",
    "\n",
    "model=Model(inputs=[input1,input2,input3],outputs=[out])\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss='mean_absolute_error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ace5a5-92ec-434e-bcdf-efbf240ef8cb",
   "metadata": {},
   "source": [
    "## To use eagerly execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "8d13d61c-2dda-4178-aa1f-b2c7b6d50332",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "a780bd28-6a2a-40e3-89e0-95b0de56f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array_np=np.array(x_array)\n",
    "y_array_np=np.array(y_array)\n",
    "x_hyper_np=np.array(x_hyper)\n",
    "x_map_np=np.array(x_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5610a256-bc60-46b2-833d-5c96d727dcac",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "329d8c01-272f-4cf9-94f4-7d499fb945a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([x_map_np,x_array_np,x_hyper_np],y_array_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fe4492-8e9e-43ee-a061-b2b09aadee74",
   "metadata": {},
   "source": [
    "# Model prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a981fa97-7a46-4846-b56d-07429ddef968",
   "metadata": {},
   "source": [
    "## Importanct functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "e157f921-a63f-4de6-951e-2e2332192535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('my_model') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e14843-1d4b-464e-a701-b26764623bf3",
   "metadata": {},
   "source": [
    "### Load model weight (Run this section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "e451013e-4eab-4a71-bc87-523bf8ed8a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Input,concatenate\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "#from keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Model\n",
    "input1=Input(shape=(60, 60, 3))\n",
    "y1=Conv2D(32, (3, 3), activation='relu', input_shape=(60, 60, 3))(input1)\n",
    "y2= MaxPooling2D()(y1)\n",
    "y3=Conv2D(64, (3, 3), activation='relu')(y2)\n",
    "y4= Flatten()(y3)\n",
    "output1=Dense(10, activation=\"relu\")(y4)\n",
    "\n",
    "#model2\n",
    "input2=Input(shape=(5,))\n",
    "z1=Dense(5, activation=\"relu\")(input2)\n",
    "#model3\n",
    "input3=Input(shape=(4,))\n",
    "x=Dense(1, activation=\"relu\")(input3)\n",
    "\n",
    "merge = concatenate([output1, z1,x])\n",
    "out=Dense(1, activation=\"relu\")(merge)\n",
    "\n",
    "#model_load=Model(inputs=[input1,input2,input3],outputs=[out])\n",
    "#opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "#model.compile(optimizer=opt, loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "9abe3a28-3337-41d4-a84f-813239fed87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_27 (InputLayer)           [(None, 60, 60, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 58, 58, 32)   896         input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, 29, 29, 32)   0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 27, 27, 64)   18496       max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 46656)        0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_28 (InputLayer)           [(None, 168)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_29 (InputLayer)           [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 10)           466570      flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 5)            845         input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 1)            5           input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 16)           0           dense_26[0][0]                   \n",
      "                                                                 dense_27[0][0]                   \n",
      "                                                                 dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 1)            17          concatenate_7[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 486,829\n",
      "Trainable params: 486,829\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('my_model')\n",
    "\n",
    "# Check its architecture\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198db3e6-50c2-4dd6-9808-027e779fa15a",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8513bc51-34cb-44c5-bf80-bc5664c862de",
   "metadata": {},
   "source": [
    "### Input formats in model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb31461-5873-4368-bfdf-39ecfc529ebc",
   "metadata": {},
   "source": [
    "- Input latitude in float format\n",
    "- Input longitude in float format\n",
    "- Input time in following format '21 March 2021 12:00AM'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2648ff-2798-450e-a0d7-d730cc066a57",
   "metadata": {},
   "source": [
    "### prediction=predict_NO2(latitude,longitude,time)\n",
    "## Example\n",
    "`prediction=predict_NO2(51.472830, -0.013733,'March 21 2021 12:00AM')`\n",
    "\n",
    "#### Note: Enter date between 9th March 2021 to 30th April 2022 (due to limitation of data given in challenge). It takes last 7 days data as time series input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "38ac5b18-ea19-4c9f-9993-1126f72e3729",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "        \n",
    "\n",
    "def predict_NO2(input_latitude,input_longitude,input_time):\n",
    "    hyper_spec_features=get_hyper_Spectral_features(input_latitude,input_longitude)\n",
    "    hyper_spec_features=np.array(hyper_spec_features)\n",
    "    hyper_spec_features=hyper_spec_features[ np.newaxis, :]\n",
    "\n",
    "    df =pd.read_csv('data1/timeseries/'+ get_coord_pointer_dist(input_latitude,input_longitude)[0])\n",
    "    df=df.dropna()\n",
    "    \n",
    "    #df.to_excel('extracted_data/timeseries.xlsx')\n",
    "   \n",
    "    df['date']=df['Date']+' '+df['Time']\n",
    "    \n",
    "    from datetime import datetime\n",
    "    df['date'] = df['date'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S'))\n",
    "    df['Nitrogen dioxide']=df['Nitrogen dioxide'].apply(lambda x:remove_bad_data(x))\n",
    "    df['Nitrogen dioxide']=get_coord_pointer_dist(51.103522,-0.604248)[1]*0.1+df['Nitrogen dioxide']\n",
    "\n",
    "    time=convert_date(input_time)\n",
    "\n",
    "    hours =168\n",
    "    hours_added = timedelta(hours = -hours)\n",
    "    hours_time_stamp = timedelta(hours = -1)\n",
    "    hours_time_stamp=time+hours_time_stamp\n",
    "    time_past=time + hours_added\n",
    "    time_past\n",
    "    x= df[df['date'] < time]#['Nitrogen dioxide']\n",
    "    x=x[x['date'] >= time_past]['Nitrogen dioxide']\n",
    "    x=list(x)\n",
    "    last_timestamp= df[df['date'] == hours_time_stamp]['Nitrogen dioxide']\n",
    "    X=[]\n",
    "    for i in x:\n",
    "        try:\n",
    "            X.append(float(i))\n",
    "        except ValueError:\n",
    "             X.append(10)\n",
    "    time_features=X\n",
    "    time_features=np.array(time_features)\n",
    "    time_features=time_features[ np.newaxis, :]\n",
    "\n",
    "    LatMin=input_latitude-0.1\n",
    "    LatMax=input_latitude+0.1\n",
    "    LngMin=input_longitude-0.1\n",
    "    LngMax=input_longitude+0.1\n",
    "    map = smopy.Map((LatMin,LngMin, LatMax,LngMax))\n",
    "    name=str(input_latitude)+'_'+str(input_longitude)+'.png'\n",
    "    map.save_png(name)\n",
    "    img = cv2.imread(name)\n",
    "    resized_image = cv2.resize(img, (60, 60)) \n",
    "    resized_image.shape\n",
    "\n",
    "    map_features=np.array(resized_image)\n",
    "    #adding axis for single batch forecasting\n",
    "    map_features=map_features[ np.newaxis, :]\n",
    "    # In case you need to add a eries of input for prediction then comment out new axis becasue tensorflow forecast\n",
    "    #in batches, a single sset of features will give uneven dimenensions. Thats why to bring them in one batch a new single axis has to be added\n",
    "    prediction=new_model.predict([map_features,time_features,hyper_spec_features])\n",
    "    final_prediction= 0.683*float(last_timestamp)+0.317*float(prediction) #Impact of last hour has 68.3% of avergae weight\n",
    "\n",
    "    result=final_prediction\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "a8eae47b-0257-4a36-826b-0858fb581d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowered zoom level to keep map size reasonable. (z = 11)\n",
      "--- 1.6261248588562012 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#import datetime\n",
    "start_time = time.time()\n",
    "#dt_started = datetime.datetime.utcnow()\n",
    "#51.522918_-0.130821\n",
    "prediction=predict_NO2(51.55650893020211, -0.35156826179036915,'March 20 2021 01:00PM')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "#dt_ended = datetime.datetime.utcnow()\n",
    "#print((dt_ended - dt_started).total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b6a92a-bfef-4c5c-8b0c-8e9886057d71",
   "metadata": {},
   "source": [
    "### Load model from saved folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "34998bb6-b2d7-46eb-b7ec-5eeaf8baf474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.086740736639763"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f0f6ee-d94a-49fb-8a8f-487dd77e80b8",
   "metadata": {},
   "source": [
    "## Thankyou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a4387d-fb5c-4b33-830f-5be709792066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
